{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G_EVAL 테스트\n",
    "\n",
    "- `g_eval_test_dataset_generation.ipynb`의 결과로 생성된 `financial_g_eval_test_dataset.csv`를 준비해주세요.\n",
    "\n",
    "- 준비된 테스트 데이터셋을 리트리버와 생성 부분을 나눠서 평가하게 됩니다.\n",
    "\n",
    "- 평가 기준은 프롬프트에 명시되어 있으며, T/F 방식으로 해당 점수를 획득 및 차감합니다.\n",
    "\n",
    "- 각 데이터마다 평가를 하게 되며 평균을 내어 전체 점수를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "def call_api(question: str) -> Dict:\n",
    "    \"\"\"API 호출하여 context와 answer 받기\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8090/api/query\",\n",
    "            json={\"query\": question},\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 에러: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_retrieval(contexts: List[str], ground_truth: str, question: str, client: OpenAI) -> Dict:\n",
    "    \"\"\"Retrieval 평가\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "다음 기준에 따라 검색된 컨텍스트(retrieved contexts)를 평가해주세요.\n",
    "\n",
    "질문: {question}\n",
    "Ground Truth 답변: {ground_truth}\n",
    "\n",
    "검색된 컨텍스트:\n",
    "---\n",
    "{json.dumps(contexts, ensure_ascii=False, indent=2)}\n",
    "---\n",
    "\n",
    "각 평가 기준에 대해 Yes(true) 또는 No(false)로 답변해주세요:\n",
    "\n",
    "1. Strong Similarity (5점):\n",
    "컨텍스트들 중 Ground Truth와 강한 유사성을 보이는 것이 있나요?\n",
    "\n",
    "2. Essential Information (5점):\n",
    "컨텍스트들이 Ground Truth의 핵심 정보를 포함하고 있나요?\n",
    "\n",
    "3. Question Addressing (4점):\n",
    "컨텍스트들이 사용자의 질문을 충분히 다루고 있나요?\n",
    "\n",
    "4. Relevance (3점):\n",
    "모든 컨텍스트가 Ground Truth 또는 질문과 관련이 있나요?\n",
    "\n",
    "5. Reasonable Length (3점):\n",
    "컨텍스트의 길이와 수가 적절하며 불필요한 정보로 사용자를 압도하지 않나요?\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 정보 검색 시스템의 성능을 평가하는 전문가입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"retrieval_evaluation\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"criteria_scores\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"strong_similarity\": {\"type\": \"boolean\"},\n",
    "                                \"essential_information\": {\"type\": \"boolean\"},\n",
    "                                \"question_addressing\": {\"type\": \"boolean\"},\n",
    "                                \"relevance\": {\"type\": \"boolean\"},\n",
    "                                \"reasonable_length\": {\"type\": \"boolean\"}\n",
    "                            },\n",
    "                            \"required\": [\"strong_similarity\", \"essential_information\", \n",
    "                                       \"question_addressing\", \"relevance\", \"reasonable_length\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"criteria_scores\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    scores = result[\"criteria_scores\"]\n",
    "    \n",
    "    # boolean을 점수로 변환\n",
    "    score_weights = {\n",
    "        \"strong_similarity\": 5,\n",
    "        \"essential_information\": 5,\n",
    "        \"question_addressing\": 4,\n",
    "        \"relevance\": 3,\n",
    "        \"reasonable_length\": 3\n",
    "    }\n",
    "    \n",
    "    total_score = sum(score_weights[key] for key, value in scores.items() if value)\n",
    "    \n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"total\": total_score\n",
    "    }\n",
    "\n",
    "def evaluate_generation(answer: str, ground_truth: str, question: str, client: OpenAI) -> Dict:\n",
    "    \"\"\"Generation 평가\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "다음 기준에 따라 생성된 답변을 평가해주세요.\n",
    "\n",
    "질문: {question}\n",
    "Ground Truth 답변: {ground_truth}\n",
    "생성된 답변: {answer}\n",
    "\n",
    "각 평가 기준에 대해 Yes(true) 또는 No(false)로 답변해주세요:\n",
    "\n",
    "1. Relevance (5점): 답변이 질문과 명확하게 관련이 있고 사용자의 의도를 반영하나요?\n",
    "2. Factual Correctness (5점): 답변이 사실적으로 정확하고 근거 없는 정보가 없나요?\n",
    "3. Completeness (5점): 답변이 질문과 ground truth에서 요구하는 모든 핵심 포인트를 포함하나요?\n",
    "4. Clarity (5점): 답변이 명확하고 간결하며 불필요한 반복이나 모호성이 없나요?\n",
    "5. Consistency (3점): 답변이 논리적으로 구성되어 있고 맥락과 일관되며 모순이 없나요?\n",
    "6. Detail Level (3점): 답변이 질문에 대해 충분한 세부 사항을 제공하되 과도하지 않나요?\n",
    "7. Citations (2점): 답변이 데이터나 주장을 참조할 때 적절한 출처나 표시를 제공하나요?\n",
    "8. Format (1점): 답변이 질문에 적합한 형식으로 제시되어 있나요?\n",
    "9. Extra Insights (1점): 답변이 사실적 정확성을 유지하면서 유용한 추가 통찰이나 맥락을 제공하나요?\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 텍스트 생성 시스템의 성능을 평가하는 전문가입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"generation_evaluation\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"criteria_scores\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"relevance\": {\"type\": \"boolean\"},\n",
    "                                \"factual_correctness\": {\"type\": \"boolean\"},\n",
    "                                \"completeness\": {\"type\": \"boolean\"},\n",
    "                                \"clarity\": {\"type\": \"boolean\"},\n",
    "                                \"consistency\": {\"type\": \"boolean\"},\n",
    "                                \"detail_level\": {\"type\": \"boolean\"},\n",
    "                                \"citations\": {\"type\": \"boolean\"},\n",
    "                                \"format\": {\"type\": \"boolean\"},\n",
    "                                \"extra_insights\": {\"type\": \"boolean\"}\n",
    "                            },\n",
    "                            \"required\": [\"relevance\", \"factual_correctness\", \"completeness\",\n",
    "                                       \"clarity\", \"consistency\", \"detail_level\", \"citations\",\n",
    "                                       \"format\", \"extra_insights\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"criteria_scores\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    scores = result[\"criteria_scores\"]\n",
    "    \n",
    "    # boolean을 점수로 변환\n",
    "    score_weights = {\n",
    "        \"relevance\": 5,\n",
    "        \"factual_correctness\": 5,\n",
    "        \"completeness\": 5,\n",
    "        \"clarity\": 5,\n",
    "        \"consistency\": 3,\n",
    "        \"detail_level\": 3,\n",
    "        \"citations\": 2,\n",
    "        \"format\": 1,\n",
    "        \"extra_insights\": 1\n",
    "    }\n",
    "    \n",
    "    total_score = sum(score_weights[key] for key, value in scores.items() if value)\n",
    "    \n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"total\": total_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "df = pd.read_csv('financial_test_dataset.csv')\n",
    "client = OpenAI()\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "results = []\n",
    "\n",
    "# 각 질문에 대해 평가 수행\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"평가 진행 중: {idx + 1}/100\")\n",
    "    \n",
    "    # API 호출\n",
    "    api_response = call_api(row['question'])\n",
    "    if not api_response:\n",
    "        continue\n",
    "        \n",
    "    # Retrieval 평가\n",
    "    retrieval_scores = evaluate_retrieval(\n",
    "        api_response['context'],\n",
    "        row['groundtruth'],\n",
    "        row['question'],\n",
    "        client\n",
    "    )\n",
    "    \n",
    "    # Generation 평가\n",
    "    generation_scores = evaluate_generation(\n",
    "        api_response['answer'],\n",
    "        row['groundtruth'],\n",
    "        row['question'],\n",
    "        client\n",
    "    )\n",
    "    \n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        'id': row['id'],\n",
    "        'question': row['question'],\n",
    "        'groundtruth': row['groundtruth'],\n",
    "        'api_answer': api_response['answer'],\n",
    "        'retrieval_score': retrieval_scores['total'],\n",
    "        'generation_score': generation_scores['total'],\n",
    "        'total_score': retrieval_scores['total'] + generation_scores['total'],\n",
    "        'retrieval_details': retrieval_scores,\n",
    "        'generation_details': generation_scores\n",
    "    })\n",
    "    \n",
    "    # API 호출 간 간격\n",
    "    time.sleep(1)\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 평균 점수 계산\n",
    "avg_retrieval = results_df['retrieval_score'].mean()\n",
    "avg_generation = results_df['generation_score'].mean()\n",
    "avg_total = results_df['total_score'].mean()\n",
    "\n",
    "print(f\"\\n평가 결과:\")\n",
    "print(f\"Retrieval 평균 점수: {avg_retrieval:.2f}/20\")\n",
    "print(f\"Generation 평균 점수: {avg_generation:.2f}/30\")\n",
    "print(f\"총점 평균: {avg_total:.2f}/50\")\n",
    "\n",
    "# 결과 저장\n",
    "results_df.to_csv('geval_results.csv', index=False)\n",
    "print(\"\\n상세 결과가 'geval_results.csv'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
