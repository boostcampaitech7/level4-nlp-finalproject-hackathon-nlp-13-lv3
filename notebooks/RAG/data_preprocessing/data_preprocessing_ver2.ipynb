{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM을 활용한 Context 보완 및 예상 질문 생성 작업\n",
    "#### data_preprocessing.ipynb를 통해 나온 final_documents.pkl 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 초기 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 중간 저장을 위한 디렉토리 설정\n",
    "CHECKPOINT_DIR = \"./data/checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 로드 및 초기 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문서 수: 3629\n",
      "\n",
      "카테고리별 문서 수:\n",
      "table: 1086\n",
      "paragraph: 1214\n",
      "footer: 306\n",
      "chart: 598\n",
      "list: 251\n",
      "figure: 167\n",
      "caption: 7\n"
     ]
    }
   ],
   "source": [
    "def load_and_verify_data(filepath: str):\n",
    "    \"\"\"데이터 로드 및 초기 상태 확인\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    \n",
    "    # 초기 상태 분석\n",
    "    total_docs = len(documents)\n",
    "    category_counts = {}\n",
    "    for doc in documents:\n",
    "        cat = doc.metadata.get('category', 'unknown')\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"총 문서 수: {total_docs}\")\n",
    "    print(\"\\n카테고리별 문서 수:\")\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"{cat}: {count}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# 실행\n",
    "documents = load_and_verify_data(\"./data/final_documents.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Summary 생성 Agent 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_agent():\n",
    "    \"\"\"Summary 생성을 위한 Agent 설정\"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    summary_prompt = \"\"\"\n",
    "    다음 내용에 대한 요약을 생성해주세요. \n",
    "    요약은 RAG 시스템에서 효과적으로 검색될 수 있도록 작성되어야 합니다.\n",
    "    \n",
    "    규칙:\n",
    "    1. 테이블, 차트, 이미지의 정보가 핵심적으로 드러나야 합니다.\n",
    "    2. 핵심 키워드와 정확한 수치 포함(수치 주의! '십억원', '억원', '천원' 등)\n",
    "    3. 객관적 사실 중심\n",
    "    \n",
    "    내용:\n",
    "    {content}\n",
    "    \n",
    "    요약:\n",
    "    \"\"\"\n",
    "    \n",
    "    return llm, summary_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Summary 생성 및 추가 프로세스:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summaries(documents: List[Document], batch_size: int = 5):\n",
    "    \"\"\"문서 요약 처리 및 메타데이터 추가\"\"\"\n",
    "    llm, prompt = create_summary_agent()\n",
    "    \n",
    "    # 처리가 필요한 문서만 필터링\n",
    "    target_categories = ['table', 'chart', 'figure']\n",
    "    docs_to_process = [doc for doc in documents if doc.metadata.get('category') in target_categories]\n",
    "    processed_count = len(docs_to_process)\n",
    "    \n",
    "    print(f\"처리 대상 문서 수: {processed_count} (table: {sum(1 for doc in docs_to_process if doc.metadata.get('category')=='table')}, \"\n",
    "          f\"chart: {sum(1 for doc in docs_to_process if doc.metadata.get('category')=='chart')}, \"\n",
    "          f\"figure: {sum(1 for doc in docs_to_process if doc.metadata.get('category')=='figure')})\")\n",
    "    \n",
    "    current_doc_idx = 0\n",
    "    \n",
    "    # 진행상황 표시를 위한 tqdm 설정\n",
    "    for i in tqdm(range(0, len(docs_to_process), batch_size), desc=\"Summary 생성\"):\n",
    "        batch = docs_to_process[i:i+batch_size]\n",
    "        \n",
    "        for doc in batch:\n",
    "            try:\n",
    "                # Summary 생성\n",
    "                response = llm.invoke(prompt.format(content=doc.page_content))\n",
    "                summary = response.content\n",
    "                \n",
    "                # 메타데이터에 summary 추가\n",
    "                new_metadata = doc.metadata.copy()\n",
    "                new_metadata['summary'] = summary\n",
    "                \n",
    "                # 원본 documents 리스트에서 해당 문서 찾아 업데이트\n",
    "                while current_doc_idx < len(documents):\n",
    "                    if documents[current_doc_idx].metadata.get('uuid') == doc.metadata.get('uuid'):\n",
    "                        documents[current_doc_idx] = Document(\n",
    "                            page_content=doc.page_content,\n",
    "                            metadata=new_metadata\n",
    "                        )\n",
    "                        current_doc_idx += 1\n",
    "                        break\n",
    "                    current_doc_idx += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n에러 발생: {e}\")\n",
    "        \n",
    "        # 배치 처리 후 중간 저장\n",
    "        if (i + batch_size) % 50 == 0:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"summary_checkpoint_{timestamp}.pkl\")\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(documents, f)\n",
    "            print(f\"\\n중간 저장 완료: {checkpoint_path}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Question Generation Agent 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_agent():\n",
    "    \"\"\"질문 생성을 위한 Agent 설정\"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    question_prompt = \"\"\"\n",
    "    주어진 내용을 바탕으로 4개의 질문을 생성해주세요.\n",
    "    \n",
    "    난이도 분포:\n",
    "    - 쉬움 (1개): 직접적인 정보 추출\n",
    "    - 중간 (2개): 정보 조합 필요\n",
    "    - 어려움 (1개): 분석/추론 필요\n",
    "    \n",
    "    답변은 반드시 주어진 내용에서 도출 가능해야 합니다.\n",
    "    \n",
    "    내용:\n",
    "    {content}\n",
    "    \n",
    "    질문 목록:\n",
    "    \"\"\"\n",
    "    \n",
    "    return llm, question_prompt\n",
    "\n",
    "def process_questions(documents: List[Document], batch_size: int = 5):\n",
    "    \"\"\"질문 생성 및 메타데이터 추가\"\"\"\n",
    "    llm, prompt = create_question_agent()\n",
    "    processed_docs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"질문 생성\"):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        \n",
    "        for doc in batch:\n",
    "            try:\n",
    "                # 질문 생성\n",
    "                response = llm.invoke(prompt.format(content=doc.page_content))\n",
    "                questions = response.content.split('\\n')\n",
    "                \n",
    "                # 메타데이터에 질문 추가\n",
    "                new_metadata = doc.metadata.copy()\n",
    "                new_metadata['expected_questions'] = questions\n",
    "                \n",
    "                processed_docs.append(Document(\n",
    "                    page_content=doc.page_content,\n",
    "                    metadata=new_metadata\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"에러 발생: {e}\")\n",
    "                processed_docs.append(doc)\n",
    "        \n",
    "        # 중간 저장\n",
    "        if (i + batch_size) % 50 == 0:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"questions_checkpoint_{timestamp}.pkl\")\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(processed_docs, f)\n",
    "            print(f\"\\n중간 저장 완료: {checkpoint_path}\")\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 최종 저장 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_verify_final_documents(documents: List[Document]):\n",
    "    \"\"\"최종 문서 저장 및 검증\"\"\"\n",
    "    # 최종 저장\n",
    "    final_path = \"./data/final_documents_ver2.pkl\"\n",
    "    with open(final_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    \n",
    "    # 검증\n",
    "    print(\"\\n=== 최종 검증 결과 ===\")\n",
    "    \n",
    "    # 문서 수 확인\n",
    "    total_docs = len(documents)\n",
    "    print(f\"총 문서 수: {total_docs}\")\n",
    "    \n",
    "    # 메타데이터 필드 확인\n",
    "    summary_count = sum(1 for doc in documents if 'summary' in doc.metadata)\n",
    "    questions_count = sum(1 for doc in documents if 'expected_questions' in doc.metadata)\n",
    "    \n",
    "    print(f\"Summary 포함 문서 수: {summary_count}\")\n",
    "    print(f\"Questions 포함 문서 수: {questions_count}\")\n",
    "    \n",
    "    # 샘플 출력\n",
    "    if documents:\n",
    "        print(\"\\n=== 샘플 문서 확인 ===\")\n",
    "        sample_doc = documents[0]\n",
    "        print(f\"카테고리: {sample_doc.metadata.get('category')}\")\n",
    "        print(f\"Summary: {sample_doc.metadata.get('summary', 'None')[:200]}...\")\n",
    "        print(\"\\nExpected Questions:\")\n",
    "        for q in sample_doc.metadata.get('expected_questions', [])[:2]:\n",
    "            print(f\"- {q}\")\n",
    "    \n",
    "    return final_path\n",
    "\n",
    "# 전체 프로세스 실행\n",
    "documents_with_summary = process_summaries(documents)\n",
    "documents_with_questions = process_questions(documents_with_summary)\n",
    "final_path = save_and_verify_final_documents(documents_with_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 작업이 끝나면 끝나면 아래와 같은 것을 실험해볼 수 있습니다.\n",
    "\n",
    "- (./data/final_documents_ver2.pkl)데이터 로드 한 후\n",
    "\n",
    "1. 메타데이터에 있는 summary를 page_content 뒷단에 추가 \n",
    "2. 메타데이터에 있는 summary를 page_content 뒷단에 추가 + 예상질문도 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1번 실험을 위한 pkl 파일 생성 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/final_documents_ver2.pkl\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    documents_ver2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_content_with_summary(documents: List[Document]) -> List[Document]:\n",
    "   \"\"\"\n",
    "   table, chart, figure 문서의 page_content에 summary 정보만 추가\n",
    "   \"\"\"\n",
    "   enhanced_documents = []\n",
    "   \n",
    "   for doc in documents:\n",
    "       # 원본 content 저장\n",
    "       original_content = doc.page_content\n",
    "       new_content_parts = []\n",
    "       \n",
    "       # Summary 추가 (table, chart, figure인 경우만)\n",
    "       if doc.metadata.get('category') in ['table', 'chart', 'figure'] and 'summary' in doc.metadata:\n",
    "           new_content_parts.append(f\"Summary:\\n{doc.metadata['summary']}\\n\")\n",
    "       \n",
    "       # 원본 content 추가\n",
    "       new_content_parts.append(original_content)\n",
    "       \n",
    "       # 새로운 Document 생성\n",
    "       enhanced_doc = Document(\n",
    "           page_content=\"\\n\".join(new_content_parts),\n",
    "           metadata=doc.metadata\n",
    "       )\n",
    "       enhanced_documents.append(enhanced_doc)\n",
    "   \n",
    "   # 처리 결과 출력\n",
    "   total_docs = len(enhanced_documents)\n",
    "   docs_with_summary = sum(1 for doc in enhanced_documents if doc.metadata.get('category') in ['table', 'chart', 'figure'] and 'summary' in doc.metadata)\n",
    "   \n",
    "   print(f\"총 처리된 문서 수: {total_docs}\")\n",
    "   print(f\"Summary가 추가된 문서 수: {docs_with_summary}\")\n",
    "   \n",
    "   # 샘플 출력 (summary가 있는 문서 중에서)\n",
    "   if docs_with_summary > 0:\n",
    "       print(\"\\n=== Summary가 추가된 문서 샘플 ===\")\n",
    "       sample_doc = next(doc for doc in enhanced_documents \n",
    "                       if doc.metadata.get('category') in ['table', 'chart', 'figure'] \n",
    "                       and 'summary' in doc.metadata)\n",
    "       print(f\"\\n카테고리: {sample_doc.metadata.get('category')}\")\n",
    "       print(f\"Content 시작 부분:\\n{sample_doc.page_content[:500]}...\")\n",
    "   \n",
    "   return enhanced_documents\n",
    "\n",
    "# 실행\n",
    "enhanced_docs = enhance_content_with_summary(documents_ver2)\n",
    "\n",
    "# 새로운 pickle 파일로 저장\n",
    "output_path = \"./data/final_documents_ver2_with_summary.pkl\"\n",
    "with open(output_path, 'wb') as f:\n",
    "   pickle.dump(enhanced_docs, f)\n",
    "\n",
    "print(f\"\\n결과 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2번 실험을 위한 pkl 파일 생성 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/final_documents_ver2.pkl\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    documents_ver2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_page_content(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    모든 문서의 page_content에 summary와 expected_questions 정보 추가\n",
    "    \"\"\"\n",
    "    enhanced_documents = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # 원본 content 저장\n",
    "        original_content = doc.page_content\n",
    "        new_content_parts = []\n",
    "        \n",
    "        # Summary 추가 (table, chart, figure인 경우)\n",
    "        if doc.metadata.get('category') in ['table', 'chart', 'figure'] and 'summary' in doc.metadata:\n",
    "            new_content_parts.append(f\"Summary:\\n{doc.metadata['summary']}\\n\")\n",
    "        \n",
    "        # Expected Questions 추가\n",
    "        if 'expected_questions' in doc.metadata:\n",
    "            new_content_parts.append(\"Expected Questions:\")\n",
    "            for question in doc.metadata['expected_questions']:\n",
    "                new_content_parts.append(f\"- {question}\")\n",
    "            new_content_parts.append(\"\")  # 빈 줄 추가\n",
    "        \n",
    "        # 원본 content 추가\n",
    "        new_content_parts.append(f\"Original Content:\\n{original_content}\")\n",
    "        \n",
    "        # 새로운 Document 생성\n",
    "        enhanced_doc = Document(\n",
    "            page_content=\"\\n\".join(new_content_parts),\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        enhanced_documents.append(enhanced_doc)\n",
    "    \n",
    "    # 처리 결과 출력\n",
    "    total_docs = len(enhanced_documents)\n",
    "    docs_with_summary = sum(1 for doc in enhanced_documents if doc.metadata.get('category') in ['table', 'chart', 'figure'] and 'summary' in doc.metadata)\n",
    "    docs_with_questions = sum(1 for doc in enhanced_documents if 'expected_questions' in doc.metadata)\n",
    "    \n",
    "    print(f\"총 처리된 문서 수: {total_docs}\")\n",
    "    print(f\"Summary가 추가된 문서 수: {docs_with_summary}\")\n",
    "    print(f\"Expected Questions가 추가된 문서 수: {docs_with_questions}\")\n",
    "    \n",
    "    # 샘플 출력\n",
    "    if enhanced_documents:\n",
    "        print(\"\\n=== 처리된 문서 샘플 ===\")\n",
    "        sample_doc = next(doc for doc in enhanced_documents if 'expected_questions' in doc.metadata)\n",
    "        print(f\"\\n카테고리: {sample_doc.metadata.get('category')}\")\n",
    "        print(f\"Content 시작 부분:\\n{sample_doc.page_content[:500]}...\")\n",
    "    \n",
    "    return enhanced_documents\n",
    "\n",
    "# 실행\n",
    "enhanced_docs = enhance_page_content(documents_ver2)\n",
    "\n",
    "# 새로운 pickle 파일로 저장\n",
    "output_path = \"./data/final_documents_ver2_with_question_summary.pkl\"\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(enhanced_docs, f)\n",
    "\n",
    "print(f\"\\n결과 저장 완료: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fertimind-C3Sc56E3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
